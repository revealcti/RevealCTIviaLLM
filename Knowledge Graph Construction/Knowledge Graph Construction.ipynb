{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import T\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pandas as pd\n",
    "def compute_full_extracted_triples(intput_sentence,keywords=None,hightlight=None,enhance=False):\n",
    "    #v2.1\n",
    "    extra_prompt=\"\"\n",
    "    extra_prompt_pair=\"\"\n",
    "    extra_prompt_hightlight=\"\"\n",
    "    if keywords!=None and len(keywords)>0 and enhance == True:\n",
    "        extra_prompt = \"When you find those keywords in above sentence, you should pay more attention to them and extract more triples about them.\"+str(keywords)\n",
    "        #if len(keywords)!=1:\n",
    "            #pairs = [f\"'{keywords[i]}' AND '{keywords[j]}'\" for i in range(len(keywords)) for j in range(i+1, len(keywords))]\n",
    "            #extra_prompt_pair =\"Rule 12, these following two subjects/objects may have hidden relationships\"+str(pairs)+\"You may create new triples based on the hidden relationships. For example, given triple [Formbook, download, plugin] and [plugin, is, XLoader], you should extract [Formbook, download, XLoader].\"\n",
    "\n",
    "    if hightlight!=None and len(hightlight)>0 and enhance == True:\n",
    "        extra_prompt_hightlight=\"Those sentences contain some important information, you should pay more attention to them and extract more triples about them.\"+str(hightlight)\n",
    "    \n",
    "    def generate_prompt_postprocess(text):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        ''' \n",
    "        You play the role of an entity extraction expert and modify/simplify/split the text (extracted multiple triples) in the entity extraction result I gave you (a python dictionary with key as the source sentence with ellipsis and value as the extracted triples) according to the following rules. A triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the physical world. A triple consists of three elements: [SUBJECT, RELATION,OBJECT]. The subject and the object are entities, which can be things, people, places, events, or abstract concepts. The relation is a relation that connects the subject and the object, and expresses how they are related. For example, [Formbook, is, malware] is a triple that describes the relationship between the malware Formbook and the concept of malware.\n",
    "        Rule 1: If the subject or object in a triple contains pronouns such as it, they, malware, Trojan, attack, ransomware, or group, replace them with a specific name as much as possible according to the context, such as “CVE-xxx” or “XLoader” will replace \"it\" or \"malware\" if context has this relationship information.\n",
    "        Rule 2: Focus on malware, Trojan horse, CVE, or hacking organization as the subject of the triples, if a subject with \"malware\" or \"Trojan horse\" or \"CVE\" or \"hacking organization\" is found and has additional suffixes, remove the suffixes.\n",
    "        Rule 3: Split a complex triple into multiple simpler forms. For example, [Formbook and XLoader, are,malware] should be split into [Formbook,is,malware] and [XLoader,is,malware].\n",
    "        Rule 4: If the [subject,relation] in a triple can be formed into a new [subject,relation,object] triple because relation itself has a new object in it, create a new triple while keeping the original one. \n",
    "        Rule 5: If the object can be simplified to a more concise, generic expression, create a new triple while keeping the original one. For example, [\"Formbook\", \"save\", \"XLoader in desktop\"] MUST has a new triple [\"Formbook\", \"save\", \"XLoader\"] due to the object \"XLoader in desktop\" can be simplified to \"XLoader\".\n",
    "        Rule 6: Simplify the subject, object, and relation into a more concise, generic expression.\n",
    "        Rule 7:When you encounter a subject or object that contains modifiers and adjectives, remove them. For example, [a notorious Formbook malware] should be simplified to [Formbook].\n",
    "        Rule 8:When you encounter a plural or past tense form, convert it to singular or present tense. For example, [Windows users] should be converted to [Windows user].\n",
    "        Rule 9:When you encounter an MD5, registry, path, or other identifier that contains prefixes, remove them. For example, [md5 xxxxx] should be simplified to [xxxxx].\n",
    "        Rule 10:When you encounter a proper noun that contains a suffix, remove the suffix. For example, [“Specific names of a malware/ransomware/trojan” malware/ransomware/trojan] should be simplified to [“Specific names of a malware/ransomware/trojan”]\n",
    "        Rule 11: Make sure the subject has a prefix \"SUBJECT:\", the relation has prefix \"RELATION:\", the object has prefix \"OBJECT:\", a triple example is \"[SUBJECT:Formbook, RELATION:save, OBJECT:a file]\n",
    "        '''\n",
    "        +extra_prompt_pair+\"Here is my entity extraction result:\\\"\"+str(text)+\"\\\".Now, you apply the rules I told you before. Write down your though, think it step by step. If all triple don't need to be modified based on specific rule, just rewrite all triples that don't need to be modified again. In the end, you MUST tell me the final new entity extraction result. Make sure your results start with \\\"[[\\\" and end with \\\"]]\\\".\"\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "    \n",
    "    def generate_prompt(text):\n",
    "            promptmessage = [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \n",
    "            ''' \n",
    "            As an AI trained in entity extraction and relationship extraction. You're an advanced AI expert, so even if I give you a complex sentence, you'll still be able to perform the relationship extraction task. The ouput of the task is a list of list.\n",
    "            A triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the physical world. A triple MUST has THREE elements: [Subject, Relation,  Object]. For example, \"[Subject:FinSpy malware, Relation:was the final payload]\"(2 elements) and \"[Subject:FinSpy malware, Relation:was, Object:the final payload, None:that will be used]\"(4 elements) do not contain exactly 3 elements and should not exist in your result. The subject and the object should be Noun. The relation should be a relation words that connects the subject and the object, and expresses how they are related. \n",
    "            \n",
    "            You only extracts triples that are related to computer science, that means focus on those sentences that explicitly involve concepts and entities related to the field of computer science. For instance, sentences mentioning programming languages, software and hardware technologies, algorithms, network protocols, security vulnerabilities, operating systems, etc., are all related to computer science. Examples of valid triples include [Formbook, is, malware] because it describes relationships between entities within the field of computer science.\n",
    "\n",
    "            Conversely, triples should not be extracted from content unrelated to computer science. For example, [The Sun, is, a star], although a valid relational description, should be excluded because it pertains to astronomy, not computer science. Similarly, [Picasso, is, a painter] should also not be considered, as it involves the field of art, not computer science.\n",
    "            \n",
    "            If my input sentences do not have any triple about computer science,you must output the answer as [[Blank placeholders since the current text does not contain any computer science related triples]]. If you find that you are keeping output triples that are not related to computer science, you should stop as soon as possible and output the result as [[Blank placeholders since the current text does not contain any computer science related triples]]\\\n",
    "            '''\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I got it.\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is one sentence from example article:\\\"Leafminer attempts to infiltrate target networks through various means of intrusion: watering hole websites, vulnerability scans of network services on the internet, and brute-force/dictionary login attempts.\\\"\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"[[SUBJECT:Leafminer,RELATION:attempts to infiltrate,OBJECT:target networks],[SUBJECT:Leafminer,RELATION:use,OBJECT:watering hole websites],[SUBJECT:Leafminer,RELATION:use,OBJECT:vulnerability scans of network services on the internet],[SUBJECT:Leafminer,RELATION:use,OBJECT:brute-force],[SUBJECT:Leafminer,RELATION:use,OBJECT:dictionary login attempts]].\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is one sentence from example article:\\\"Kismet is also a powerful tool for penetration testers that need to better understand their target and perform wireless LAN discovery.\\\"\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"[[SUBJECT:Kismet,RELATION:is a powerful tool for, OBJECT:penetration testers],[SUBJECT:testers, RELATION:understand, OBJECT:their target],[SUBJECT:testers,RELATION: perform, OBJECT:wireless LAN discovery]]\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is one sentence from example article:\\\"Legendary Pokémon , or Pokémon Illusions are extremely rare and often very powerful Pokémon that are often associated with legends of creation and/or destruction within their endemic regions. \\\"\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"This text does not contain any computer science related triples, my output is [[Blank placeholders since the current text does not contain any computer science related triples]]\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Here is one sentence from example article:\\\"The Royal Knights  are a group of thirteen Mega-level[1] Holy Warrior Digimon[2] that are the Digital World's sacred guardians,[3] and are famed among Digimon as guardian deities of the Computer Network.[4][5] The group was founded by Imperialdramon Paladin Mode,\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"[[The Royal Knights, are, a group of thirteen Mega-level Holy Warrior Digimon],[The Royal Knights, are, the Digital World's sacred guardians],[The Royal Knights, are, guardian deities of the Computer Network],[The group, was founded by, Imperialdramon Paladin Mode]]. I find that the sentence does not contain any computer science related triples, and I am keeping output triples that are not related to computer science. So I stop here and output the result as [[Blank placeholders since the current text does not contain any computer science related triples]].\"\n",
    "            },\n",
    "            {\"role\": \"user\",\n",
    "            \"content\": \n",
    "            \"\"\"\n",
    "            Here are my new sentence, extract all possible entity triples from it. Now, I start to give you sentence.\\\"\"\n",
    "            \"\"\"+text+\n",
    "            \"\"\"\\\"Now, my input text are over. You MUST follow the rules I told you before. \n",
    "            \"\"\"+extra_prompt+extra_prompt_hightlight\n",
    "            },\n",
    "            ]\n",
    "            return promptmessage\n",
    "\n",
    "    def generate_prompt_basedon3(inSent,inlist):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":\"Your primary objective is to consolidate entity extraction results derived from sentence or sentences by difference distinct assistants into a singular, coherent output. This process necessitates a careful analysis to identify and merge triples that, despite potentially differing in phrasing, communicate identical information. Additionally, it is imperative to filter out any triples that fail to meet the specified criteria for validity and relevance.Key Points: Eliminate Invalid Triples:Any triples not conforming to the structure of having exactly one SUBJECT, one RELATION, and one OBJECT should be removed.Exclude triples missing any of the three essential components [SUBJECT, RELATION, OBJECT] .Discard any triples that introduce information not explicitly mentioned in the provided sentence.Consolidation means maperge triples that convey the same information but are expressed using different terminologies or structures. This involves combining synonyms or semantically similar phrases that refer to the same entities or actions.Examples for Reference: Consider the sentence FinSpy is a surveillance software designed by Gamma Group to spy on digital communications. Analysis of extraction results from different assistants might yield the following triples: Assistant A: [FinSpy, designed by, Gamma Group], [FinSpy, to spy on, digital communications] Assistant B: [FinSpy, is designed by, Gamma Group], [Surveillance software, is used for, spying on digital communications] Assistant C: [FinSpy, developed by, Gamma Group], [FinSpy, monitors, communications] Unified and Consolidated Output: [FinSpy, designed by, Gamma Group] [FinSpy, to spy on, digital communications] This output exemplifies the elimination of redundant or irrelevant triples and the merging of equivalent triples into a single, consolidated form. Now, the source sentences for extracted triples tasks are'+\"+str(inSent)+', the extracted triples result are'+str(inlist)+\"Just answer me your final answer without any other words, and only start with \\\"[[\\\", and only end with \\\"]]\\\".\"\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "        \n",
    "    def clean_text(text):\n",
    "        import string,re\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        cleaned_text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "        cleaned_text = re.sub(r'[\\s{}]+'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'SUBJECT|RELATION|OBJECT', '', cleaned_text)\n",
    "        return cleaned_text if cleaned_text else 'Null'\n",
    "\n",
    "    def get_only_triples(text):\n",
    "        import re\n",
    "        text = text.replace('\"', '').replace(\"'\", \"\")\n",
    "        text = text.replace('\\n', '')\n",
    "        text = re.sub(r'\\[\\s+', '[', text)\n",
    "        text = re.sub(r'\\s+\\]', ']', text)\n",
    "        text = re.sub(r',\\s+', ',', text)\n",
    "        text = re.sub(r'\\[\\s+', '[', text)\n",
    "        if '[[' in text and ']]' in text:\n",
    "            return text[text.rindex('[['):text.rindex(']]')+2]\n",
    "        return text\n",
    "\n",
    "    single_sentence=intput_sentence\n",
    "    \n",
    "    content_first_extraction=''\n",
    "    first_answer_list=[]\n",
    "    temperature_list=[0.2]*3\n",
    "    from openai import OpenAI\n",
    "    api_key = \"EMPTY\"\n",
    "    api_base = \"http://localhost:8000/v1\"\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    import concurrent.futures\n",
    "    print(generate_prompt(single_sentence))\n",
    "    def process_temperature(temperature):\n",
    "        stream = client.chat.completions.create(\n",
    "            model='./cti-qwen1.5-70b-awq',\n",
    "            messages=generate_prompt(single_sentence),\n",
    "            stream=True,\n",
    "            max_tokens=6000,\n",
    "            temperature=temperature,\n",
    "            extra_body={\n",
    "            \"stop_token_ids\": [7]\n",
    "            }\n",
    "        )\n",
    "        final_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                final_response += chunk.choices[0].delta.content\n",
    "        content_first_extraction = final_response\n",
    "        cleaned_text = clean_text(str(content_first_extraction))\n",
    "        if any(keyword in cleaned_text for keyword in ['CVExxx', 'Formbook', 'XLoader', 'Malwaresavetextfile', 'Leafminer', 'FinSpy', 'Kismet', 'Specificnamesofa']):\n",
    "            return 'ERROR'\n",
    "        else:\n",
    "            return get_only_triples(content_first_extraction)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        first_answer_list = list(executor.map(process_temperature, temperature_list))\n",
    "    \n",
    "    def check_placeholders_in_list(sublist, placeholder=\"Blank placeholders\"):\n",
    "\n",
    "        count = sum(1 for item in sublist[:3] if placeholder in item)\n",
    "                return count >= 2\n",
    "    \n",
    "    blank_flag=check_placeholders_in_list(first_answer_list)\n",
    "  \n",
    "    if blank_flag :\n",
    "        extracted_text='This text does not contain any computer science related triples.'\n",
    "    else:\n",
    "        stream = client.chat.completions.create(\n",
    "                model='./cti-qwen1.5-70b-awq',\n",
    "                messages=generate_prompt_basedon3(single_sentence,first_answer_list[0:3]),\n",
    "                stream=True,\n",
    "                max_tokens=8000,\n",
    "                temperature=0.5,\n",
    "                extra_body={\n",
    "                \"stop_token_ids\": [7]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        final_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                final_response += chunk.choices[0].delta.content  \n",
    "        content_first_extraction_merged = final_response\n",
    "        content_first_extraction_merged=get_only_triples(content_first_extraction_merged)\n",
    "        stream = client.chat.completions.create(\n",
    "                model='./cti-qwen1.5-70b-awq',\n",
    "                messages=generate_prompt_postprocess(content_first_extraction_merged),\n",
    "                stream=True,\n",
    "                max_tokens=10000,\n",
    "                temperature=0.7,\n",
    "                extra_body={\n",
    "                \"stop_token_ids\": [7]\n",
    "                }\n",
    "            )\n",
    "        final_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                #print(chunk.choices[0].delta.content, end=\"\")\n",
    "                final_response += chunk.choices[0].delta.content\n",
    "        content_simple_version=final_response  \n",
    "        extracted_text = get_only_triples(content_simple_version)\n",
    "    \n",
    "    return extracted_text\n",
    "\n",
    "def clean_full_extracted_triples(text):\n",
    "    #remvoe all \\n\n",
    "    text=text.replace('\\n','')\n",
    "    #remove the space between ‘:\" and \"[\"\n",
    "    import re\n",
    "    text=re.sub(r':\\s+\\[',r'[',text)\n",
    "    #remove the space between ‘[\" and \"[\"\n",
    "    text=re.sub(r'\\s+\\[',r'[',text)\n",
    "    #remove the space between ‘]\" and \"]\"\n",
    "    text=re.sub(r'\\s+\\]',r']',text)\n",
    "    #replace ], ] or ],] or ] ,] with ]]\n",
    "    text = re.sub(r'\\]\\s*,\\s*\\]', ']]', text)\n",
    "\n",
    "    triple_only_text=text\n",
    "    #if [[ and ]] in text, extract the content between them with a [ and ]\n",
    "    if \"[[\" in text and \"]]\" in text:\n",
    "        start_index = text.rindex('[[')+1\n",
    "        end_index = text.rindex(']]') + 1\n",
    "        triple_only_text = text[start_index:end_index]\n",
    "    else:\n",
    "        if \"[[\"in text or \"]]\" in text:\n",
    "            start_index = text.index('[')\n",
    "            end_index = text.rindex(']') + 1\n",
    "            triple_only_text = text[start_index:end_index]\n",
    "    #remove all \" and ' in text\n",
    "    triple_only_text=triple_only_text.replace('\"','')\n",
    "    triple_only_text=triple_only_text.replace(\"'\",'')\n",
    "\n",
    "    return triple_only_text\n",
    "\n",
    "def merge_extracted_triples(longmem,shortmem,sentence):\n",
    "    #v1.5\n",
    "    def generate_prompt(longmem,shortmem,sentence):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        '''You are a triples integration assistant. Triple is a basic data structure, which describes concepts and their relationships. A triple in long-term and short-term memory MUST has THREE elements: [Subject, Relation, Object]. You are now reading a whole article and extract all triples from it. But you can only see part of the article at a time. In order to record all the triples from a article, you have the following long-term memory area to record the triples from the entire article. long-term memory stores information on the aricle parts you have already read.\n",
    "        -The start of the long-term memory area-\n",
    "        #Triples will be added here\n",
    "        -The end of the short-term memory area-\n",
    "        Second, you now see a part of this article. Based on this part, you already extract such triples and place them in your short-term memory: \n",
    "        -The start of the short-term memory area-\n",
    "        #Triples will be added here\n",
    "       -The end of the short-term memory area-\n",
    "        Third, now review your long-term memory and short-term memory. Modify the short-term memory into a new short-term memory. You should follow following rules to modify triples in short-term memory to make them consistent with triples in long-term memory. You should write down how you use the rule to modify the triples in short-term memory. \n",
    "        \n",
    "        Rule 1. You notice that in these triples, some triples have subjects and objects that contain partially identical terms and refer to the same specific nouns, but these specific nouns have prefixes/suffixes/modifiers that make them not identical. You should delete the prefixes/suffixes/modifiers and unify them into the same specific nouns.\n",
    "        \n",
    "        Before rule: [the Formbook, is designed to run as, a deleter] [Formbook sample, is designed to run as, one-time encryptor]\n",
    "\n",
    "        After rule: [Formbook, is designed to run as, a deleter] [Formbook, is designed to run as, one-time encryptor]\n",
    "\n",
    "        Explanation: The words \"the Formbook\" and \"Formbook sample\" refer to the same entity, so they are unified to use the exact same subject \"Formbook\" for consistency.\n",
    "        \n",
    "        Rule 2. Be especially careful that when you meet specific names of malware,CVE, Trojans, hacker organizations, etc., always use their specific names and remove the prefixes/suffixes/modifiers.\n",
    "        \n",
    "        Before rule: [Malware Formbook, is, malware] \n",
    "        \n",
    "        After rule: [Formbook, is, malware]\n",
    "        \n",
    "        Explanation: The word \"Formbook\" is a specific name of malware, so it should be used as the subject of the triple and the prefix \"Malware\" should be removed.\n",
    "        \n",
    "        Rule 3. Don't add unexisting triples to your new short-term memory. \n",
    "    \n",
    "        Suppose you find in long-term memory: [the malware, download, Leafminer] and in short-term memory: [Formbook, is, malware]. You cannot add a new triple in new short term memory: [Formbook, download, Leafminer]. Because you don't have evidence that \"the malware\" in the long-term memory specifically refers to \"Formbook\".\n",
    "        \n",
    "        Rule 4. Don't add unexisting triples that don't exsit in long-term memory or short-term memory to your new short-term memory. You should add triples from long-term memory or short-term memory to your new short-term memory, not from your imagination and selfcreation\n",
    "        \n",
    "        Rule 5. Don't add any example word like 'Formbook','XLoader','Leafminer', 'FinSpy', 'Kismet' in your new short-term memory area, they are just example words not the real triples in the long term memory area or short term memory area.\n",
    "        \n",
    "        Rule 6. new short-term memory area must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. A triple in new short-term memory MUST has THREE elements: [Subject, Relation, Object]. \n",
    "        '''\n",
    "        },\n",
    "        {\"role\": \"assistant\",'content': 'Yes, I understand and totally follow the rules.'},\n",
    "        {\"role\": \"user\",'content': '''\n",
    "         -The start of the long-term memory area-  \n",
    "         [SUBJECT:AgumonHacker, RELATION:disrupts, OBJECT:network operations], [SUBJECT:HackGroup Gabumon, RELATION:targets, OBJECT:financial institutions], [SUBJECT:Malware Biyomon, RELATION:steals, OBJECT:personal data], [SUBJECT:CyberAttack 2042, RELATION:compromised, OBJECT:database systems],  [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload]\n",
    "         -The end of the long-term memory area-\n",
    "\n",
    "        -The start of the short-term memory area-  \n",
    "         [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group], [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns], [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom], [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks],[SUBJECT:Biyomon, RELATION:run, OBJECT:payload]\n",
    "        -The end of the short-term memory area-\n",
    "         '''\n",
    "         },\n",
    "        {\"role\": \"assistant\",'content': \n",
    "         '''\n",
    "         Modifications required based on the rules:\n",
    "        Apply Rule 1 and Rule 2: Unify subjects and objects with partially identical terms and remove any prefixes/suffixes/modifiers, particularly for specific names like malware and hacker groups.\n",
    "        Modified short-term memory:\n",
    "\n",
    "        [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group] → [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group]\n",
    "        [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns] → [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns]\n",
    "        [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom] → [SUBJECT:Malware Biyomon, RELATION:encrypts, OBJECT:files for ransom]\n",
    "        [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks] → [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks]\n",
    "\n",
    "        I also find a triple in long-term memory that needs to be modified based on Rule 1 and Rule 2:\n",
    "\n",
    "        [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload] → [SUBJECT:Biyomon, RELATION:save, OBJECT:payload]\n",
    "\n",
    "        Apply Rule 3 and 4 and 5: We must ensure we do not create unexisting triples when merging the memories.\n",
    "        After applying the modifications, we now create a new short-term memory area:\n",
    "\n",
    "        -The start of new short-term memory area- \n",
    "        [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group], \n",
    "        [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns],\n",
    "        [SUBJECT:Biyomon, RELATION:encrypts, OBJECT:files for ransom],\n",
    "        [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks],\n",
    "        [SUBJECT:Biyomon, RELATION:run, OBJECT:payload],\n",
    "        [SUBJECT:Biyomon, RELATION:save, OBJECT:payload] \n",
    "        -The end of new short-term memory area-\n",
    "         '''   },\n",
    "        {\"role\": \"user\",'content': \n",
    "        '''\n",
    "        Good. Now, let's swtich to another article. \n",
    "        -The start of the long-term memory area-\n",
    "        '''+str(longmem)+'''\n",
    "        -The end of the long-term memory area-\n",
    "    \n",
    "        -The start of the short-term memory area-\n",
    "        '''+str(shortmem)+'''\n",
    "        -The end of the short-term memory area-\n",
    "        \n",
    "        Now, follow the rules. Write down how you use the rule to modify the triples in short-term memory. If there is no any triple in my input short-term memory, you still need to write down \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. with only one blank line between them. If my input short-term memory is already perfect, you still need to write down \\'-The start of new short-term memory area-\\' and content of that perfect short-term memory and ended with \\'-The end of new short-term memory area-\\'.\n",
    "        '''\n",
    "        },      \n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "    import os,openai\n",
    "    openai.api_base = \"http://localhost:8080/v1\"\n",
    "    openai.api_key = \"\"\n",
    "    model = \"Empty\"\n",
    "    import concurrent.futures\n",
    "    \n",
    "    def clean_text(text):\n",
    "        import string,re\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        cleaned_text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "        cleaned_text = re.sub(r'[\\s{}]+'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'SUBJECT|RELATION|OBJECT', '', cleaned_text)\n",
    "        return cleaned_text if cleaned_text else 'Null'\n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    retry_times=0\n",
    "    pass_flag=False\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    api_key = \"EMPTY\"\n",
    "    api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    \n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"./cti-qwen1.5-70b-awq\",\n",
    "        messages=generate_prompt(longmem,shortmem,sentence),\n",
    "        stream=True,\n",
    "        max_tokens=10000,\n",
    "        temperature=0.7,\n",
    "        extra_body={\n",
    "        \"stop_token_ids\": [7]\n",
    "        }\n",
    "    )\n",
    "    final_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            final_response += chunk.choices[0].delta.content  \n",
    "    fullanswer=final_response\n",
    "   \n",
    "    return fullanswer   \n",
    "\n",
    "def check_brackets(my_string):\n",
    "    if my_string is None or len(my_string) == 0:\n",
    "        return False\n",
    "\n",
    "    my_string = my_string.strip()\n",
    "\n",
    "    first_ten_chars = my_string[:10]\n",
    "    last_ten_chars = my_string[-10:]\n",
    "    \n",
    "    found_bracket_in_first_ten = '[' in first_ten_chars\n",
    "\n",
    "    found_bracket_in_last_ten = ']' in last_ten_chars\n",
    "\n",
    "    return found_bracket_in_first_ten and found_bracket_in_last_ten\n",
    "\n",
    "def full_text_to_parts(text):\n",
    "    import nltk\n",
    "    paragraphs = text.split('\\n')\n",
    "    \n",
    "    processed_paragraphs = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) > 600:\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            sentences = [x[0:500] for x in sentences]\n",
    "            new_paragraph = ''\n",
    "            for sentence in sentences:\n",
    "                temp_length = len(new_paragraph) + len(sentence)\n",
    "                if temp_length < 600:\n",
    "                    new_paragraph += (sentence + '\\n')\n",
    "                else:\n",
    "                    if len(new_paragraph) >= 20:\n",
    "                        processed_paragraphs.append(new_paragraph.strip())\n",
    "                    new_paragraph = sentence + '\\n'\n",
    "            if len(new_paragraph) >= 20:\n",
    "                processed_paragraphs.append(new_paragraph.strip())\n",
    "        else:\n",
    "            if len(paragraph) >= 20:\n",
    "                processed_paragraphs.append(paragraph.strip())\n",
    "\n",
    "    combined_paragraphs = []\n",
    "    current_combined = ''\n",
    "\n",
    "    for paragraph in processed_paragraphs:\n",
    "        temp_length = len(current_combined) + len(paragraph) + 1 \n",
    "        if temp_length < 600:\n",
    "            current_combined += (' ' if current_combined else '') + paragraph\n",
    "        else:\n",
    "            combined_paragraphs.append(current_combined)\n",
    "            current_combined = paragraph\n",
    "            \n",
    "    if current_combined:\n",
    "        combined_paragraphs.append(current_combined)\n",
    "\n",
    "    return combined_paragraphs\n",
    "\n",
    "def name_entity_recognition(text):\n",
    "  def ask(prompt, token, temp, model,streamprint=True):\n",
    "      import os\n",
    "      from openai import OpenAI\n",
    "      if model == \"gpt4\":\n",
    "          os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "          api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "          api_base = 'https://api.openai.com/v1'\n",
    "          setmodel = ''\n",
    "      else:\n",
    "          setmodel=model\n",
    "          api_key = \"EMPTY\"\n",
    "          api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "      client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "      print(\"Model:\", setmodel)\n",
    "      stream = client.chat.completions.create(\n",
    "          model=setmodel,\n",
    "          messages=prompt,\n",
    "          stream=True,\n",
    "          max_tokens=token,\n",
    "          temperature=temp,\n",
    "          extra_body={\n",
    "          \"stop_token_ids\": [7]\n",
    "          }\n",
    "      )\n",
    "      final_response = \"\"\n",
    "      for chunk in stream:\n",
    "          if chunk.choices[0].delta.content is not None:\n",
    "              if streamprint:\n",
    "                  print(chunk.choices[0].delta.content, end=\"\")\n",
    "              final_response += chunk.choices[0].delta.content  \n",
    "      return final_response                \n",
    "  text=text[0:4096]\n",
    "  input=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": '''Please execute the following steps to extract all proper nouns from the input text:\n",
    "\n",
    "Step 1: Identification\n",
    "\n",
    "    Identify all proper nouns that are specific to the field of computer science.\n",
    "\n",
    "Inclusion Criteria (Rule1):\n",
    "\n",
    "    Only include proper nouns that are must directly mentioned in the text.\n",
    "    Focus on specific names related to:\n",
    "        Malware\n",
    "        Unique vulnerabilities (specifically referenced by CVE identifiers)\n",
    "        Distinct network identifiers (IP addresses, domain names, URLs)\n",
    "        Detailed Indicators of Compromise (IOCs) such as file paths, hash values, port numbers, registry key changes, MAC addresses, and unique signatures.\n",
    "        Software names (e.g., antivirus programs, hacking tools, operating systems)\n",
    "        Hardware names (e.g., specific models of routers, servers, or IoT devices)\n",
    "        Specific names of hacker groups or threat actors\n",
    "        Specific names of security protocols or encryption algorithms\n",
    "        Specific names of programming languages, libraries, or frameworks\n",
    "        Specific names of computer science concepts or technologies\n",
    "        \n",
    "Step 2: Exclusion\n",
    "    Remove all:\n",
    "        Human names\n",
    "        Country names\n",
    "        City names\n",
    "\n",
    "Step 3: Output Formatting\n",
    "\n",
    "    Provide the output in the format of a Python list: [\"name1\", \"name2\", ...]. \n",
    "    Make sure every name should be enclosed in double quotes and separated by a comma!\n",
    "    After listing all names according to Rule1, evaluate each against Rule2.\n",
    "    For each name you exclude based on Rule2, provide a concise justification. If you don't exclude any names, you still start output with \"Final list based on Rule2: []\".\n",
    "\n",
    "Final Step: Final Output\n",
    "\n",
    "    After the evaluation, present the final, curated list that adheres to Rule1 and Rule2.'''\n",
    "      \n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Understood. I will now proceed to extract the names.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"ExampleMalware1 is a malware that is designed to run as a deleter in Country1. ExampleMalware1 sample is designed to run as one-time encryptor. It is an update version of ExampleMalware2 from human1.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"[\\\"ExampleMalware1\\\", \\\"ExampleMalware2\\\", \\\"Country1\\\", \\\"human1\\\"]. Then I list every name and think if each name should be excluded based on Rule2.\\n ExampleMalware1: Include.\\n ExampleMalware2: Include. Country1: Exclude,country name.\\n.\\n human1: Exclude, human name.\\nFinal list based on Rule2: [\\\"ExampleMalware1\\\", \\\"ExampleMalware2\\\"].\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Good job! Now, I will give you another new sentence. Here is the sentence: \\\"\"+text+\"\\\"\"}\n",
    "  ]\n",
    "  import ast,json\n",
    "  max_retries = 2\n",
    "  for attempt in range(max_retries):\n",
    "    try:\n",
    "      answer = ask(input, 4096, 1, \"./cti-qwen1.5-70b-awq\", False)\n",
    "      if 'Rule2' in answer:\n",
    "        list_str_answer = answer[answer.rindex('Rule2'):]\n",
    "      else:\n",
    "        list_str_answer = answer\n",
    "      list_str_answer = list_str_answer[list_str_answer.rindex('['):list_str_answer.rindex(']')+1]\n",
    "      print('\\nName entity recognition result raw:', answer)\n",
    "      if '\\'' not in list_str_answer and '\\\"' not in list_str_answer:\n",
    "          stripped_string = list_str_answer.strip('[]')\n",
    "          elements = stripped_string.split(', ')\n",
    "          quoted_elements = [\"'{}'\".format(element) for element in elements]\n",
    "          list_str_answer = '[' + ', '.join(quoted_elements) + ']'\n",
    "      print('\\nName entity recognition result with only list:', list_str_answer)\n",
    "      answer = ast.literal_eval(str(list_str_answer))\n",
    "      return answer\n",
    "\n",
    "    except Exception as e: \n",
    "      print(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "      if attempt == max_retries - 1:  \n",
    "          return ['BlankWordHere']\n",
    "\n",
    "    return ['BlankWordHere']\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import pandas as pd  # Assuming longmem_cache is a pandas DataFrame\n",
    "\n",
    "def save_to_unique_filename(article_raw_fulltext, longmem_cache):\n",
    "    def generate_unique_filename_from_string(input_string):\n",
    "        # Trim spaces from the ends of the string\n",
    "        trimmed_string = input_string.strip()\n",
    "\n",
    "        # Keep only the first 196 characters including only alphabet letters and spaces\n",
    "        filtered_chars = [c for c in trimmed_string if c.isalpha() or c.isspace()]\n",
    "        first_196_chars = ''.join(filtered_chars[:196])\n",
    "\n",
    "        # Replace spaces with underscores\n",
    "        base_filename = 'triples_' + first_196_chars.replace(' ', '_') + '.xlsx'\n",
    "\n",
    "        # Check if the filename is unique\n",
    "        if not os.path.exists(base_filename):\n",
    "            return base_filename  # If it does not exist, return the original filename\n",
    "\n",
    "        # Separate filename and extension\n",
    "        file_name, file_extension = os.path.splitext(base_filename)\n",
    "\n",
    "        # Attempt to create a unique filename by appending a counter\n",
    "        for counter in range(2, 1000):  # Try from 2 to 999\n",
    "            new_filename = f\"{file_name}_step{counter}{file_extension}\"\n",
    "            if not os.path.exists(new_filename):\n",
    "                return new_filename  # Return new filename if it does not exist\n",
    "\n",
    "        # If a unique filename is not found, use a random letter\n",
    "        random_letter = random.choice(string.ascii_letters)\n",
    "        new_filename = f\"{file_name}_step{random_letter}{file_extension}\"\n",
    "        return new_filename\n",
    "\n",
    "    # Generate a unique filename based on the article text\n",
    "    unique_filename = generate_unique_filename_from_string(article_raw_fulltext)\n",
    "\n",
    "    # Try to save the DataFrame to an Excel file\n",
    "    try:\n",
    "        longmem_cache.to_excel(unique_filename, index=False)\n",
    "        print(f\"Save to: {unique_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Save fail with: {e}\")\n",
    "\n",
    "\n",
    "def full_article_to_longmem(single_article,enhance=False,realtime_othernlp=True):\n",
    "    resolved_text=single_article \n",
    "    import concurrent.futures\n",
    "    from collections import Counter\n",
    "    def run_parallel_entity_recognition(text):\n",
    "        results = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = [executor.submit(name_entity_recognition, text) for _ in range(3)]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                results.append(set(future.result()))\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            print(f'Results {i+1}:', result)\n",
    "        \n",
    "        entity_counts = Counter()\n",
    "        for result in results:\n",
    "            entity_counts.update(result)\n",
    "        \n",
    "        common_entities = {entity for entity, count in entity_counts.items() if count >= 2}\n",
    "        \n",
    "        discard_entities = {entity for entity, count in entity_counts.items() if count < 2}\n",
    "        print('Common Entities:', common_entities)\n",
    "        print('Discard Entities:', discard_entities)\n",
    "\n",
    "        return list(common_entities)\n",
    "\n",
    "    keywords = run_parallel_entity_recognition(resolved_text)\n",
    "    print('Keywords are:',keywords)\n",
    "    #Get the dict_string_label here\n",
    "    #if realtime_othernlp is True\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences = sent_tokenize(resolved_text)\n",
    "    dict_string_label={}\n",
    "    import pickle\n",
    "    from simpletransformers.classification import MultiLabelClassificationModel\n",
    "    pkl_file = open('big_label_list.pkl', 'rb')\n",
    "    big_label_list = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    if realtime_othernlp is True: \n",
    "        print('Realtime Simpletransformers')\n",
    "        my_best_model_dir='./tactic model'\n",
    "        model = MultiLabelClassificationModel(\"roberta\", my_best_model_dir, num_labels=len(big_label_list))\n",
    "        for sentence in sentences:\n",
    "            predictions, raw_outputs = model.predict([sentence])\n",
    "            predicted_labels = [big_label_list[i] for i, label in enumerate(predictions[0]) if label == 1]\n",
    "            dict_string_label[sentence] = predicted_labels\n",
    "    else:\n",
    "        print('Load from offline sentence predictions')\n",
    "        with open('offline_predictions_simple.pkl', 'rb') as f:\n",
    "            dict_sentence_onehotlabel = pickle.load(f)\n",
    "        for singlesentence in sentences:\n",
    "            if singlesentence in dict_sentence_onehotlabel.keys():\n",
    "                onehotlabel=dict_sentence_onehotlabel[singlesentence]\n",
    "                predicted_labels=[big_label_list[i] for i, label in enumerate(onehotlabel) if label == 1]\n",
    "                dict_string_label[singlesentence]=predicted_labels\n",
    "            else:\n",
    "                print('No prediction for this sentence:',singlesentence)\n",
    "  \n",
    "    grouped_texts_strings = full_text_to_parts(single_article)\n",
    "    triple_cache = []\n",
    "    text_cache = []\n",
    "    for i in range(len(grouped_texts_strings)):\n",
    "        if i >40:\n",
    "            return\n",
    "        this_time_test=grouped_texts_strings[i]\n",
    "        if len(this_time_test) > 1500:\n",
    "            this_time_test = this_time_test[0:1500]\n",
    "        print('Working on：',this_time_test)\n",
    "        this_keywords = []\n",
    "        for single_keyword in keywords:\n",
    "            if single_keyword.lower() in this_time_test.lower():\n",
    "                this_keywords.append(single_keyword)\n",
    "                \n",
    "        highlight_sentence = []\n",
    "        for key in dict_string_label.keys():\n",
    "            if key.lower() in this_time_test.lower():\n",
    "                highlight_sentence.append(key)\n",
    "                \n",
    "        if len(this_keywords) != 0 and enhance:\n",
    "            print('Find keywords：',this_keywords)\n",
    "        else:\n",
    "            print('No keywords found, normal operation')\n",
    "        if len(highlight_sentence) != 0 and enhance:\n",
    "            print('Find CTI sentence label：',highlight_sentence)\n",
    "        else:\n",
    "            print('No CTI sentence label found, normal operation')\n",
    "            \n",
    "        triple = compute_full_extracted_triples(this_time_test, this_keywords, highlight_sentence,enhance)\n",
    "            \n",
    "        print('Get triples：',triple)\n",
    "        clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "        if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False:\n",
    "            triple = compute_full_extracted_triples(this_time_test, this_keywords, highlight_sentence,enhance)\n",
    "            clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "            \n",
    "        if 'This text does not contain any computer science related triples'  in triple or 'Blank placeholders since the current text does not contain any computer science related triples' in triple:\n",
    "            clean_triple_forMEM='This text does not contain any computer science related triples'\n",
    "        else:\n",
    "            if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False:\n",
    "                triple = compute_full_extracted_triples(this_time_test)\n",
    "                clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "            \n",
    "            if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False:\n",
    "                triple = compute_full_extracted_triples(this_time_test)\n",
    "                clean_triple_forMEM = triple\n",
    "            \n",
    "        print('Get short-term memory :')\n",
    "        print(clean_triple_forMEM)\n",
    "        \n",
    "        if i == 0:\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                longmem = clean_triple_forMEM\n",
    "            else:\n",
    "                longmem = 'No longterm memory'\n",
    "            triple_cache.append(clean_triple_forMEM)\n",
    "            text_cache.append(this_time_test)\n",
    "            print('Finish the first paragraph')\n",
    "            import pandas as pd\n",
    "            try:\n",
    "                new_data = pd.DataFrame({'single_article': [str(single_article)], 'longmem': [str(longmem)],'totalkeywords':[str(keywords)],'totalhighlightsentence':[str(dict_string_label)],'thiskeywords':[str(this_keywords)],'thishighlightsentence':[str(highlight_sentence)]})\n",
    "            except:\n",
    "                new_data = pd.DataFrame({'single_article': [str(single_article)], 'longmem': [str(longmem)]})\n",
    "            save_to_unique_filename(single_article, new_data)\n",
    "        if i> 40:\n",
    "            print('Too many paragraphs, stop here')\n",
    "        if i >= 1 and i<=40:\n",
    "            print('Start to merge the short-term memory to long-term memory')\n",
    "            print(longmem)\n",
    "            original_longmem=longmem\n",
    "            if len(longmem)>=1000:\n",
    "                    longmem=longmem[-1000:]\n",
    "                    if '[' in longmem:\n",
    "                        longmem=longmem[longmem.index('['):]\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                max_retries = 3  # 最大重试次数\n",
    "                retry_count = 0  # 重试计数器\n",
    "                while retry_count < max_retries:\n",
    "                    newlongmem = merge_extracted_triples(longmem, clean_triple_forMEM, this_time_test)\n",
    "                    print('newlongmem is:')\n",
    "                    print(newlongmem)\n",
    "                    newlongmem=newlongmem.replace('-The start of the new short-term memory area-','-The start of new short-term memory area-')\n",
    "                    newlongmem=newlongmem.replace('-The end of the new short-term memory area-','-The end of new short-term memory area-') \n",
    "                    if '-The start of new short-term memory area-' in newlongmem and '-The end of new short-term memory area-' in newlongmem:\n",
    "                        newlongmem=newlongmem[newlongmem.rindex('-The start of new short-term memory area-')+len('-The start of new short-term memory area-'):newlongmem.rindex('-The end of new short-term memory area-')]\n",
    "                        if not any(keyword in newlongmem for keyword in ['Formbook', 'XLoader', 'savetextfile', 'Leafminer', 'FinSpy', 'Kismet','Agumon','Gabumon','Biyomon','2042']):\n",
    "                            longmem = str(original_longmem)+', '+str(newlongmem)\n",
    "                            retry_count=9999\n",
    "                        else:\n",
    "                            retry_count += 1\n",
    "                    else:\n",
    "                        retry_count += 1\n",
    "            else:\n",
    "                longmem=original_longmem\n",
    "            print('get new longmem')\n",
    "            print(longmem)      \n",
    "            import pandas as pd\n",
    "            try:\n",
    "                new_data = pd.DataFrame({'single_article': [str(single_article)], 'longmem': [str(longmem)],'totalkeywords':[str(keywords)],'totalhighlightsentence':[str(dict_string_label)],'thiskeywords':[str(this_keywords)],'thishighlightsentence':[str(highlight_sentence)]})\n",
    "            except:\n",
    "                new_data = pd.DataFrame({'single_article': [str(single_article)], 'longmem': [str(longmem)]})\n",
    "                \n",
    "            longmem_cache = new_data\n",
    "            \n",
    "            \n",
    "            import os\n",
    "            import random\n",
    "            import string\n",
    "            \n",
    "            def generate_unique_filename_from_string(input_string):\n",
    "                trimmed_string = input_string.strip()\n",
    "\n",
    "                filtered_chars = [c for c in trimmed_string if c.isalpha() or c.isspace()]\n",
    "                first_64_chars = ''.join(filtered_chars[:196])\n",
    "\n",
    "                base_filename = 'triples_'+first_64_chars.replace(' ', '_') + '.xlsx'\n",
    "                if not os.path.exists(base_filename):\n",
    "                    return base_filename  \n",
    "\n",
    "                file_name, file_extension = os.path.splitext(base_filename)\n",
    "                \n",
    "                for counter in range(2, 1000):  \n",
    "                    new_filename = f\"{file_name}_step{counter}{file_extension}\"\n",
    "                    if not os.path.exists(new_filename):\n",
    "                        return new_filename \n",
    "                \n",
    "                random_letter = random.choice(string.ascii_letters)\n",
    "                new_filename = f\"{file_name}_step{random_letter}{file_extension}\"\n",
    "                return new_filename\n",
    "            \n",
    "            full_article_filename = generate_unique_filename_from_string(input_string=single_article)\n",
    "            \n",
    "            full_path = full_article_filename\n",
    "\n",
    "            try:\n",
    "                longmem_cache.to_excel(full_path, index=False)\n",
    "                print(f\"Save to：{full_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Get error：{e}\")\n",
    "                \n",
    "    return longmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "import pandas as pd\n",
    "target= pd.read_csv('target.csv')\n",
    "targettext=target.loc[0,'string']\n",
    "input_articles=target['string'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start extracting from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "def process_article(article, index):\n",
    "    try:\n",
    "        full_article_to_longmem(article,enhance=True,realtime_othernlp=True)\n",
    "        with open(\"done.txt\", \"a\") as file:\n",
    "            file.write(article + \"\\n\")\n",
    "        with open(\"done_index.txt\", \"a\") as file:\n",
    "            file.write(str(index))\n",
    "    except Exception as e:\n",
    "        with open(\"error.txt\", \"a\") as file:\n",
    "            file.write(article + \"\\n\")\n",
    "        print(f\"Failed on：{e}\")\n",
    "\n",
    "\n",
    "pool = ThreadPoolExecutor(max_workers=8)\n",
    "for i, article in enumerate(input_articles):\n",
    "    try:\n",
    "        pool.submit(process_article, article, i)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on：{e}\")\n",
    "        continue\n",
    "\n",
    "pool.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpupower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
