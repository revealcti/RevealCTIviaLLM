{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Package And Function, Load Data\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from math import log, e\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "def filter_graph_by_behave_conf(graph):\n",
    "    new_graph = nx.Graph()\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"behave_conf\", 0) > 0.5:\n",
    "            new_graph.add_edge(edge[0], edge[1], **graph[edge[0]][edge[1]])\n",
    "    new_graph.remove_nodes_from(list(nx.isolates(new_graph)))\n",
    "    return new_graph\n",
    "\n",
    "def get_graph_tactics(graph):\n",
    "    tactic_list = []\n",
    "    for edge in graph.edges:\n",
    "        one_hot = graph[edge[0]][edge[1]].get(\"tactic_conf\")\n",
    "        tactic_this = [i for i, x in enumerate(one_hot) if x == 1]\n",
    "        tactic_list.append(tactic_this)\n",
    "    return tactic_list\n",
    "\n",
    "def check_unique_tactic(tactic_list):\n",
    "    total_tactic_list = set()\n",
    "    for single_tactic_list in tactic_list:\n",
    "        for tactic in single_tactic_list:\n",
    "            total_tactic_list.add(tactic)\n",
    "    return len(total_tactic_list)\n",
    "\n",
    "def check_graph_by_unique_article_id(graph):\n",
    "    id_set = set()\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"article_id\") is not None:\n",
    "            id_set.add(graph[edge[0]][edge[1]][\"article_id\"])\n",
    "    if len(id_set) >= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_graph_has_tactic(graph):\n",
    "    for edge in graph.edges:\n",
    "        if graph[edge[0]][edge[1]].get(\"tactic_conf\") is not None:\n",
    "            if \"1\" in str(graph[edge[0]][edge[1]][\"tactic_conf\"]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def new_draw_graph(\n",
    "    graph, draw_edges=True, save_folder=\"defaultgraph\", saveorshow=\"show\"\n",
    "):\n",
    "    pos = nx.circular_layout(graph)\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=100)\n",
    "    nx.draw_networkx(\n",
    "        graph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=20,\n",
    "        arrowsize=90,\n",
    "        linewidths=1.5,\n",
    "        arrowstyle=\"->\",\n",
    "        edge_color=\"red\",\n",
    "        node_shape=\"o\",\n",
    "        bbox=dict(facecolor=\"black\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"),\n",
    "        node_color=\"black\",\n",
    "        font_size=15,\n",
    "        font_color=\"white\",\n",
    "    )\n",
    "    if draw_edges:\n",
    "        edge_labels = nx.get_edge_attributes(graph, \"relation\")\n",
    "        articles_labels = nx.get_edge_attributes(graph, \"article_id\")\n",
    "        one_hot = nx.get_edge_attributes(graph, \"tactic_conf\")\n",
    "        true_tactic = {}\n",
    "        for key in one_hot.keys():\n",
    "            true_tactic[key] = [i for i, x in enumerate(one_hot[key]) if x == 1]\n",
    "        data = articles_labels\n",
    "        article_dict = {}\n",
    "        article_id = 0\n",
    "        for key, value in data.items():\n",
    "            if value not in article_dict:\n",
    "                article_dict[value] = \"from article\" + str(article_id)\n",
    "                article_id = article_id + 1\n",
    "        result_dict = {}\n",
    "        for key, value in data.items():\n",
    "            result_dict[value] = article_dict[value]\n",
    "        for key, value in edge_labels.items():\n",
    "            new_value = \"relation:\" + value\n",
    "            article_true_value = articles_labels[key]\n",
    "            article_012_value = result_dict[article_true_value]\n",
    "            new_value += f\"\\n{article_012_value}\"\n",
    "            if key in true_tactic:\n",
    "                if len(true_tactic[key]) > 0:\n",
    "                    new_value = new_value + \"\\nattack tactic:\\n\"\n",
    "                    for tactic in true_tactic[key]:\n",
    "                        new_value += f\"{big_label_list[tactic]},\\n\"\n",
    "            edge_labels[key] = new_value\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            graph,\n",
    "            pos,\n",
    "            edge_labels=edge_labels,\n",
    "            font_color=\"red\",\n",
    "            font_size=12,\n",
    "        )\n",
    "    if saveorshow == \"show\":\n",
    "        plt.show()\n",
    "    if saveorshow == \"save\":\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "        plt.savefig(save_folder + \"/\" + str(random.randint(0, 1000000)) + \".png\")\n",
    "        plt.close()\n",
    "\n",
    "def calculate_percentage(folder_path):\n",
    "    total_count = 0\n",
    "    meaningful_count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            total_count += 1\n",
    "            if filename[-5] == \"y\":\n",
    "                meaningful_count += 1\n",
    "            elif filename[-5] == \"n\":\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Warning: unexpected filename {filename}\")\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return meaningful_count / total_count * 100\n",
    "\n",
    "def entropy(lst):\n",
    "    n = len(lst)\n",
    "    counts = {}\n",
    "    for x in lst:\n",
    "        counts[x] = counts.get(x, 0) + 1\n",
    "    probs = [count / n for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "def entropy_one_hot(list):\n",
    "    if len(list) == 0:\n",
    "        return 0\n",
    "    arr = np.array(list)\n",
    "    n_rows, n_cols = arr.shape\n",
    "    ent = 0\n",
    "    for i in range(n_cols):\n",
    "        col = arr[:, i]\n",
    "        ones = np.count_nonzero(col)\n",
    "        zeros = n_rows - ones\n",
    "        p_ones = ones / n_rows\n",
    "        p_zeros = zeros / n_rows\n",
    "        if p_ones > 0 and p_zeros > 0:\n",
    "            ent += -p_ones * log(p_ones, e) - p_zeros * log(p_zeros, e)\n",
    "    return ent\n",
    "\n",
    "def calculate_graph_stats(graph):\n",
    "    nodes_data = pd.DataFrame(graph.nodes(data=True), columns=[\"node\", \"data\"])\n",
    "    edges_data = pd.DataFrame(\n",
    "        graph.edges(data=True), columns=[\"source\", \"target\", \"data\"]\n",
    "    )\n",
    "    nodes_data_df = pd.DataFrame(graph.nodes(data=True), columns=[\"node\", \"data\"])\n",
    "    entity_count = (\n",
    "        nodes_data_df[\"data\"].apply(lambda x: x.get(\"entity_conf\") == 1).sum()\n",
    "    )\n",
    "    behave_count = (\n",
    "        edges_data[\"data\"]\n",
    "        .apply(lambda x: x.get(\"behave_conf\") and x.get(\"behave_conf\") > 0.5)\n",
    "        .sum()\n",
    "    )\n",
    "    tactic_count = (\n",
    "        edges_data[\"data\"]\n",
    "        .apply(lambda x: x.get(\"tactic_conf\") and x.get(\"tactic_conf\") != [0] * 10)\n",
    "        .sum()\n",
    "    )\n",
    "    entity_percent = entity_count / len(graph.nodes()) if len(graph.nodes()) > 0 else 0\n",
    "    behave_percent = behave_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    tactic_percent = tactic_count / len(graph.edges()) if len(graph.edges()) > 0 else 0\n",
    "    avg_precent = (\n",
    "        (entity_percent + behave_percent + tactic_percent) / 3\n",
    "        if len(graph.nodes()) > 0\n",
    "        else 0\n",
    "    )\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"entity_percent\": [entity_percent],\n",
    "            \"behave_percent\": [behave_percent],\n",
    "            \"tactic_percent\": [tactic_percent],\n",
    "        }\n",
    "    )\n",
    "    all_article_id = edges_data[\"data\"].apply(lambda x: x.get(\"article_id\")).tolist()\n",
    "    all_tactic = edges_data[\"data\"].apply(lambda x: x.get(\"tactic_conf\")).tolist()\n",
    "    article_entropy = entropy(all_article_id)\n",
    "    tactic_entropy = entropy_one_hot(all_tactic)\n",
    "    return (\n",
    "        avg_precent,\n",
    "        entity_percent,\n",
    "        behave_percent,\n",
    "        tactic_percent,\n",
    "        article_entropy,\n",
    "        tactic_entropy,\n",
    "    )\n",
    "\n",
    "def calculate_community_scores(listofcommunities, inputG):\n",
    "    df_community_and_scores = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"community_nodes\",\n",
    "            \"entity_percent\",\n",
    "            \"behave_percent\",\n",
    "            \"tactic_percent\",\n",
    "            \"avg_precent\",\n",
    "            \"article_entropy\",\n",
    "        ]\n",
    "    )\n",
    "    listofcommunities = listofcommunities.copy()\n",
    "    np.random.shuffle(listofcommunities)\n",
    "    for one_community in (listofcommunities):\n",
    "        graph_one_community = create_new_graph(one_community, inputG)\n",
    "\n",
    "        (\n",
    "            avg_precent,\n",
    "            entity_percent,\n",
    "            behave_percent,\n",
    "            tactic_percent,\n",
    "            article_entropy,\n",
    "            tactic_entropy,\n",
    "        ) = calculate_graph_stats(graph_one_community)\n",
    "\n",
    "        df_community_and_scores = pd.concat(\n",
    "            [\n",
    "                df_community_and_scores,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"community_nodes\": [one_community],\n",
    "                        \"entity_percent\": [entity_percent],\n",
    "                        \"behave_percent\": [behave_percent],\n",
    "                        \"tactic_percent\": [tactic_percent],\n",
    "                        \"avg_precent\": [avg_precent],\n",
    "                        \"article_entropy\": [article_entropy],\n",
    "                        \"tactic_entropy\": [tactic_entropy],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    min_x = df_community_and_scores[\"article_entropy\"].min()\n",
    "    max_x = df_community_and_scores[\"article_entropy\"].max()\n",
    "    df_community_and_scores[\"normalized_article_entropy\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"article_entropy\"]\n",
    "        z_i = (x_i - min_x) / (max_x - min_x)\n",
    "        df_community_and_scores.loc[i, \"normalized_article_entropy\"] = z_i\n",
    "\n",
    "    min_x = df_community_and_scores[\"tactic_entropy\"].min()\n",
    "    max_x = df_community_and_scores[\"tactic_entropy\"].max()\n",
    "    df_community_and_scores[\"normalized_tactic_entropy\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"tactic_entropy\"]\n",
    "        if max_x == min_x:\n",
    "                    z_i = 0\n",
    "        else:\n",
    "            z_i = (x_i - min_x) / (max_x - min_x)        \n",
    "        df_community_and_scores.loc[i, \"normalized_tactic_entropy\"] = z_i\n",
    "\n",
    "    df_community_and_scores[\"avg_score\"] = (\n",
    "        df_community_and_scores[\"entity_percent\"]\n",
    "        + df_community_and_scores[\"behave_percent\"]\n",
    "        + df_community_and_scores[\"tactic_percent\"]\n",
    "        + df_community_and_scores[\"normalized_article_entropy\"]\n",
    "        + df_community_and_scores[\"normalized_tactic_entropy\"]\n",
    "    ) / 5\n",
    "\n",
    "    df_community_and_scores = df_community_and_scores.sort_values(\n",
    "        by=[\"avg_score\"], ascending=False\n",
    "    )\n",
    "\n",
    "    min_x = df_community_and_scores[\"avg_score\"].min()\n",
    "    max_x = df_community_and_scores[\"avg_score\"].max()\n",
    "    df_community_and_scores[\"normalized_avg_score\"] = 0\n",
    "    for i, row in df_community_and_scores.iterrows():\n",
    "        x_i = row[\"avg_score\"]\n",
    "        if max_x == min_x:\n",
    "            z_i = 0\n",
    "        else:\n",
    "            z_i = (x_i - min_x) / (max_x - min_x)\n",
    "        df_community_and_scores.loc[i, \"normalized_avg_score\"] = z_i\n",
    "\n",
    "    return df_community_and_scores\n",
    "\n",
    "def create_new_graph(node_list, graph):\n",
    "    new_graph = graph.subgraph(node_list).copy()\n",
    "    isolated_nodes = list(nx.isolates(new_graph))\n",
    "    new_graph.remove_nodes_from(isolated_nodes)\n",
    "    return new_graph\n",
    "\n",
    "def get_community_size(folder_name):\n",
    "    return int(folder_name.split(\"_\")[-1])\n",
    "\n",
    "###Load Data\n",
    "big_label_list=['Initial Access', 'Execution', 'Defense Evasion', 'Command and Control', 'Privilege Escalation', 'Persistence','Lateral Movement','DataLeak','Exfiltration','Impact']\n",
    "with open('/home/local/XXXXAD/user/Dropbox (XXXX)/code/G_Value_forXXXX.pkl', 'rb') as f:\n",
    "    G_Value = pickle.load(f)\n",
    "communities = {}\n",
    "\n",
    "files1 = [f for f in os.listdir('.') if f.startswith('communities_value_') and f.endswith('.pkl')]\n",
    "files2 = [f for f in os.listdir('.') if f.startswith('ownAlgorithm_threshold_') and f.endswith('.pkl')]\n",
    "files=files1+files2\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'rb') as f:\n",
    "        community = pickle.load(f)\n",
    "        communities[file] = community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTIKG Community Detection: Base Function\n",
    "import community\n",
    "import random\n",
    "from collections import deque\n",
    "import networkx as nx\n",
    "import os\n",
    "def clear_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    with open(file_path, 'w') as f:\n",
    "        pass\n",
    "\n",
    "def CTIKG_Community_Detection_GetEdgesIntoDict(inputG):\n",
    "    edges = {}\n",
    "    for t in inputG.edges():\n",
    "        if len(t) > 0:\n",
    "            if t[0] != t[1]:\n",
    "                if t[0] not in edges:\n",
    "                    edges[t[0]] = {t[1]}\n",
    "                else:\n",
    "                    edges[t[0]].add(t[1])\n",
    "                if t[1] not in edges:\n",
    "                    edges[t[1]] = {t[0]}\n",
    "                else:\n",
    "                    edges[t[1]].add(t[0])\n",
    "    return edges\n",
    "\n",
    "def CTIKG_Community_Detection_FirstPartition(edges,first_part_file):\n",
    "    OUT = open(first_part_file, \"w\")\n",
    "    node_count = 0\n",
    "    for n in edges:\n",
    "        node_count = node_count + 1\n",
    "        if node_count > 0:\n",
    "            index = {}\n",
    "            reverse_index = {}\n",
    "            count = 0\n",
    "            to_add_edges = []\n",
    "            adj = set([])\n",
    "            for neighbor in edges[n]:\n",
    "                index[count] = neighbor\n",
    "                reverse_index[neighbor] = count\n",
    "                adj.add(neighbor)\n",
    "                count = count + 1\n",
    "            for m in reverse_index:\n",
    "                for k in edges[m]:\n",
    "                    if k in reverse_index and reverse_index[k] < reverse_index[m]:\n",
    "                        to_add_edges.append((reverse_index[m], reverse_index[k]))\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from([i for i in index])\n",
    "            G.add_edges_from(to_add_edges)\n",
    "            if len(to_add_edges) > 0:\n",
    "                dict_H = community.best_partition(G)\n",
    "                H = {}\n",
    "                for node in dict_H:\n",
    "                    if dict_H[node] not in H:\n",
    "                        H[dict_H[node]] = set([])\n",
    "                    H[dict_H[node]].add(node)\n",
    "                for i in H:\n",
    "                    comm = H[i]\n",
    "                    if len(comm) > 0:\n",
    "                        for c in comm:\n",
    "                            OUT.write(str(index[int(c)]) + \" \")\n",
    "                        OUT.write(str(n))\n",
    "                        OUT.write(\"\\n\")\n",
    "                    elif len(comm) > 0:\n",
    "                        for c in comm:\n",
    "                            if index[int(c)] in edges[n]:\n",
    "                                OUT.write(str(index[int(c)]) + \" \")\n",
    "                        OUT.write(str(n))\n",
    "                        OUT.write(\"\\n\")\n",
    "    OUT.close()\n",
    "\n",
    "def CTIKG_Community_Detection_Jaccard(set1, set2):\n",
    "        set1 = set(set1)\n",
    "        set2 = set(set2)\n",
    "        return float(len(set1.intersection(set2))) / float(len(set1.union(set2)))\n",
    "\n",
    "def CTIKG_Community_Detection_GetMembership(first_part_file, membership_file):\n",
    "    node_membership = {}\n",
    "    IN = open(first_part_file, \"rb\")\n",
    "    read_line = IN.readline()\n",
    "    count = 0\n",
    "    while read_line:\n",
    "        t = read_line.rstrip().split()\n",
    "        if len(t) >= min_comm_size:\n",
    "            for mem in t:\n",
    "                if mem not in node_membership:\n",
    "                    node_membership[mem] = set([])\n",
    "                node_membership[mem].add(count)\n",
    "        count = count + 1\n",
    "        read_line = IN.readline()\n",
    "\n",
    "    IN.close()\n",
    "\n",
    "    OUT = open(membership_file, \"w\")\n",
    "    for n in node_membership:\n",
    "        in_comms = node_membership[n]\n",
    "        OUT.write(str(n) + \" \")\n",
    "        for c in in_comms:\n",
    "            OUT.write(str(c) + \" \")\n",
    "        OUT.write(\"\\n\")\n",
    "    OUT.close()\n",
    "\n",
    "\n",
    "def get_subset_graph(in_undirected_G, percent_to_keep):\n",
    "    num_nodes = len(in_undirected_G.nodes())\n",
    "    num_edges = len(in_undirected_G.edges())\n",
    "    num_nodes_to_keep = int(num_nodes * percent_to_keep)\n",
    "    num_edges_to_keep = int(num_edges * percent_to_keep)\n",
    "    nodes_to_keep = random.sample(list(in_undirected_G.nodes()), num_nodes_to_keep)\n",
    "    edges_to_keep = random.sample(list(in_undirected_G.edges()), num_edges_to_keep)\n",
    "    G_Value_undirected_subset = nx.Graph()\n",
    "    G_Value_undirected_subset.add_nodes_from(nodes_to_keep)\n",
    "    G_Value_undirected_subset.add_edges_from(edges_to_keep)\n",
    "    return G_Value_undirected_subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTIKG Community Detection: Type-based Grouping\n",
    "import random\n",
    "import networkx as nx\n",
    "first_part_file = \"./tmp/part1_.txt\"\n",
    "membership_file = \"./tmp/membership_.txt\"\n",
    "sim_file = \"./tmp/simfile_.txt\"\n",
    "clear_file(first_part_file)\n",
    "clear_file(membership_file)\n",
    "clear_file(sim_file)\n",
    "edges = CTIKG_Community_Detection_GetEdgesIntoDict(G_Value_undirected_subset)\n",
    "min_comm_size=3\n",
    "sim_threshold=0.25\n",
    "global_overlap_threshold=0.25\n",
    "CTIKG_Community_Detection_FirstPartition(edges,first_part_file)\n",
    "def CTIKG_Community_Detection_Filter_only_MalwareCVEActor(file_path, in_entity_set):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(' ')\n",
    "        if line[-1] in in_entity_set:\n",
    "            filtered_lines.append(line)\n",
    "    if os.path.exists(file_path + '.bak'):\n",
    "        os.remove(file_path + '.bak')\n",
    "    with open(file_path + '.bak', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "    with open(file_path, 'w') as f:\n",
    "        for line in filtered_lines:\n",
    "            f.write(' '.join(line) + '\\n')\n",
    "    print('The length of original file:', len(lines), 'The length of filtered file:', len(filtered_lines), 'The difference:', len(lines) - len(filtered_lines))\n",
    "\n",
    "CTIKG_Community_Detection_Filter_only_MalwareCVEActor(first_part_file, entity_conf_set)\n",
    "CTIKG_Community_Detection_GetMembership(first_part_file, membership_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTIKG Community Detection: Community Fusion\n",
    "import pickle\n",
    "with open('/home/local/XXXXAD/user/Dropbox (XXXX)/code/tmp/part1_.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "lines_result=[]\n",
    "with open('Embedding_BY_BERT.pickle', 'rb') as handle:\n",
    "    embeddings_dict = pickle.load(handle)\n",
    "for i in lines:\n",
    "    lines_result.append(embeddings_dict[i])\n",
    "    \n",
    "lines_result_possible=[]\n",
    "lines_result_possible_truevalue=[]\n",
    "for i in range(len(lines_result)):\n",
    "    if lines[i].count(' ')>=2:\n",
    "        lines_result_possible_truevalue.append(lines[i])\n",
    "        lines_result_possible.append(lines_result[i])\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "dist = pdist(lines_result_possible, metric=cosine_similarity)\n",
    "\n",
    "dist_mat = squareform(dist)\n",
    "\n",
    "sims = np.mean(dist_mat, axis=0)\n",
    "\n",
    "#\n",
    "plt.hist(sims, bins=50, color='blue', edgecolor='black')\n",
    "plt.xlabel('Cosine similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Similarity distribution')\n",
    "plt.show()\n",
    "\n",
    "threshold_V=0.15\n",
    "\n",
    "def threshold_percentile(sims, percentile):\n",
    "    threshold = sorted(sims, reverse=True)[int(len(sims) * percentile / 100)]\n",
    "    return threshold\n",
    "import os\n",
    "if os.path.exists('tmp/simfile_.txt'):\n",
    "    os.remove('tmp/simfile_.txt')\n",
    "\n",
    "value_p1file_index={}\n",
    "for line_this in lines_result_possible_truevalue:\n",
    "    index_loc=lines.index(line_this)\n",
    "    value_p1file_index[line_this]=index_loc\n",
    "    \n",
    "result_newp1=[]\n",
    "total_sim=[]\n",
    "for i in range(len(dist_mat)):\n",
    "    for j in range(i,len(dist_mat)):\n",
    "        total_sim.append(dist_mat[i][j])\n",
    "\n",
    "threshold_this=threshold_percentile(total_sim, threshold_V)\n",
    "print('threshold_this:',threshold_this)\n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(dist_mat))):\n",
    "    for j in range(i,len(dist_mat)):\n",
    "        if i!=j:\n",
    "            if dist_mat[i][j]>threshold_this:\n",
    "                aindex=value_p1file_index[lines_result_possible_truevalue[i]]\n",
    "                bindex=value_p1file_index[lines_result_possible_truevalue[j]]\n",
    "                aindex_split=lines_result_possible_truevalue[i].split(' ')\n",
    "                bindex_split=lines_result_possible_truevalue[j].split(' ')\n",
    "                aindex_split[-1]=aindex_split[-1].replace('\\n','')\n",
    "                bindex_split[-1]=bindex_split[-1].replace('\\n','')\n",
    "                union_len=len(set(aindex_split).intersection(set(bindex_split)))\n",
    "                a_len=len(aindex_split)\n",
    "                b_len=len(bindex_split)\n",
    "                if a_len <10 and b_len <10 and union_len>=1:\n",
    "                    result_newp1.append(str(aindex)+' '+str(bindex)+' '+str(dist_mat[i][j])+' '+str(union_len)+' '+str(a_len)+' '+str(b_len)+' '+str(dist_mat[i][j]))\n",
    "#save the result as txt file as tmp/simfile_.txt\n",
    "with open('tmp/simfile_.txt', 'w') as f:\n",
    "    for item in result_newp1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "def CTIKG_Community_Detection_SecondPartition(overlap_threshold, sim_file, first_part_file):\n",
    "        return_vals =CTIKG_Community_Detection_ModClusteringSingleBig(\n",
    "            sim_file, first_part_file, overlap_threshold\n",
    "        )\n",
    "        return return_vals\n",
    "\n",
    "def CTIKG_Community_Detection_ModClusteringSingleBig(sim_file, first_part_file, overlap_threshold=0):\n",
    "    overlap_threshold=global_overlap_threshold\n",
    "    IN = open(sim_file, \"r\")\n",
    "    read_line = IN.readline()\n",
    "    num_lines = 0\n",
    "    comm_edges = {}\n",
    "    to_add_edges = []\n",
    "    while read_line:\n",
    "        t = read_line.rstrip().split()\n",
    "        num_lines += 1\n",
    "        if len(t) > 0:\n",
    "            node1 = int(t[0])\n",
    "            node2 = int(t[1])\n",
    "            sim = t[2]\n",
    "            overlap = t[3]\n",
    "            if node1 not in comm_edges:\n",
    "                comm_edges[node1] = set([])\n",
    "            if node2 not in comm_edges:\n",
    "                comm_edges[node2] = set([])\n",
    "            if node2 not in comm_edges[node1]:\n",
    "                comm_edges[node1].add(node2)\n",
    "                comm_edges[node2].add(node1)\n",
    "                weight = sim\n",
    "                to_add_edges.append((node1, node2, {\"weight\": float(weight)}))\n",
    "\n",
    "        read_line = IN.readline()\n",
    "    IN.close()\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(comm_edges)))\n",
    "    G.add_edges_from(to_add_edges)\n",
    "    dict_H = community.best_partition(G)\n",
    "    H1 = {}\n",
    "    for e in dict_H:\n",
    "        if dict_H[e] not in H1:\n",
    "            H1[dict_H[e]] = set([])\n",
    "        H1[dict_H[e]].add(e)\n",
    "    H = []\n",
    "    for e in H1:\n",
    "        H.append(H1[e])\n",
    "\n",
    "    IN = open(first_part_file, \"rb\")\n",
    "    line_offset = {}\n",
    "    offset = 0\n",
    "    count = 0\n",
    "    for line in IN:\n",
    "        line_offset[count] = offset\n",
    "        count = count + 1\n",
    "        offset += len(line)\n",
    "    IN.close()\n",
    "\n",
    "    IN = open(first_part_file, \"rb\")\n",
    "    all_comms = {}\n",
    "    i = 0\n",
    "    for big_comm in H:\n",
    "        comm_members = {}\n",
    "        for comm in big_comm:\n",
    "            IN.seek(line_offset[int(comm)])\n",
    "            read_line = IN.readline()\n",
    "            t = read_line.rstrip().split()\n",
    "            if len(t) > 0:\n",
    "                for t1 in t:\n",
    "                    if t1 not in comm_members:\n",
    "                        comm_members[t1] = 0\n",
    "                    comm_members[t1] += 1\n",
    "        all_comms[i] = set([])\n",
    "        for t1 in comm_members:\n",
    "            if comm_members[t1] >= 0:\n",
    "                all_comms[i].add(t1)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return all_comms\n",
    "\n",
    "def CTIKG_Community_Detection_GetModComms(G):\n",
    "    dict_H = community.best_partition(G)\n",
    "    H1 = {}\n",
    "    for e in dict_H:\n",
    "        if dict_H[e] not in H1:\n",
    "            H1[dict_H[e]] = set([])\n",
    "        H1[dict_H[e]].add(e)\n",
    "    H = []\n",
    "    for e in H1:\n",
    "        H.append(H1[e])\n",
    "    return H\n",
    "\n",
    "def CTIKG_Community_Detection_CleanComms(to_clean):\n",
    "    comms = {}\n",
    "    count = 0\n",
    "    idx = {}\n",
    "    for t in to_clean.values():\n",
    "        if len(t) > 0:\n",
    "            comms[count] = set(t)\n",
    "            for i in t:\n",
    "                if i not in idx:\n",
    "                    idx[i] = set([])\n",
    "\n",
    "                idx[i].add(count)\n",
    "            count += 1\n",
    "        elif len(t) > 0:\n",
    "            comms[count] = set(t)\n",
    "            count += 1\n",
    "    coms = []\n",
    "    for i in range(count):\n",
    "        C = comms[i]\n",
    "        if len(C) > 0:\n",
    "            poss = set([])\n",
    "            found = 0\n",
    "            for n in C:\n",
    "                poss = poss.union(idx[n])\n",
    "            for j in poss:\n",
    "                if j < i:\n",
    "                    if (\n",
    "                        len(comms[j]) == len(comms[i])\n",
    "                        and len(comms[j].difference(comms[i])) == 0\n",
    "                    ):\n",
    "                        found = 1\n",
    "            if found != 1:\n",
    "                coms.append([t.decode(\"utf-8\") for t in C])\n",
    "        else:\n",
    "            coms.append([t.decode(\"utf-8\") for t in C])\n",
    "    return coms\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "sim_file='tmp/simfile_.txt'\n",
    "first_part_file='tmp/part1_.txt'\n",
    "global_overlap_threshold=0\n",
    "return_vals = CTIKG_Community_Detection_SecondPartition(0, sim_file, first_part_file)\n",
    "coms = CTIKG_Community_Detection_CleanComms(return_vals)\n",
    "print('Find communities number:',len(coms))\n",
    "\n",
    "import pickle\n",
    "fianlname='ownAlgorithm_threshold_'+str(threshold_V)+'.pkl'\n",
    "with open(fianlname, 'wb') as f:\n",
    "    pickle.dump(coms, f)\n",
    "print('save as pkl file name:',fianlname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTIKG Community Detection: Community Filters\n",
    "import tqdm\n",
    "for community in tqdm.tqdm(coms):\n",
    "    graph_com=create_new_graph(community,G_Value)\n",
    "    article_id=check_graph_by_unique_article_id(graph_com)\n",
    "    tactic_number=check_unique_tactic(get_graph_tactics(\n",
    "        graph_com))\n",
    "    if tactic_number>=3 and article_id:\n",
    "        count=count+1\n",
    "        new_draw_graph(graph_com,save_folder='xxxxx',saveorshow='save')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
